import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from diffusers import StableDiffusion3Pipeline, DDPMScheduler
from diffusers.optimization import get_scheduler
from peft import LoraConfig, get_peft_model
from accelerate import Accelerator
from accelerate.logging import get_logger
from accelerate.utils import set_seed
from PIL import Image
import os
from pathlib import Path
from tqdm import tqdm
import argparse
import numpy as np
import datetime
import math

logger = get_logger(__name__)


class XrayDatasetDual(Dataset):
    """
    åŒæ–‡ä»¶å¤¹Xå…‰ç‰‡æ•°æ®é›† - ç›´æ¥ä½¿ç”¨é¢„å¤„ç†å¥½çš„æ•°æ®
    - data/train/X-ray: åŸå›¾
    - data/train/X-ray_transform_padding512512: å¢å¼ºå›¾ç‰‡
    """
    def __init__(self, instance_data_root, tokenizer_1, tokenizer_2, resolution=1024):
        self.resolution = resolution
        self.tokenizer_1 = tokenizer_1
        self.tokenizer_2 = tokenizer_2
        
        # åŸå›¾æ–‡ä»¶å¤¹
        self.original_dir = os.path.join(instance_data_root, 'train', 'X-ray')
        self.original_files = sorted([f for f in os.listdir(self.original_dir) 
                                      if f.lower().endswith(('.jpg', '.png', '.jpeg'))])
        self.num_original = len(self.original_files)
        
        # å¢å¼ºå›¾ç‰‡æ–‡ä»¶å¤¹
        self.augmented_dir = os.path.join(instance_data_root, 'train', 'X-ray_transform_padding512512')
        self.augmented_files = sorted([f for f in os.listdir(self.augmented_dir)
                                       if f.lower().endswith(('.jpg', '.png', '.jpeg'))])
        self.num_augmented = len(self.augmented_files)
        
        # æ€»é•¿åº¦ = åŸå›¾ + å¢å¼ºå›¾
        self._length = self.num_original + self.num_augmented
        
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ“ Dataset Configuration:")
        logger.info(f"   Original images: {self.num_original}")
        logger.info(f"   Augmented images: {self.num_augmented}")
        logger.info(f"   Total samples: {self._length}")
        logger.info(f"   Data expansion: {self._length / self.num_original:.1f}x")
        logger.info(f"{'='*60}\n")
        
        if self._length == 0:
            raise ValueError(f"No images found in {instance_data_root}")
    
    def __len__(self):
        return self._length
    
    def __getitem__(self, idx):
        # å‰åŠéƒ¨åˆ†æ˜¯åŸå›¾ï¼ŒååŠéƒ¨åˆ†æ˜¯å¢å¼ºå›¾
        if idx < self.num_original:
            # åŠ è½½åŸå›¾
            img_path = os.path.join(self.original_dir, self.original_files[idx])
            image = Image.open(img_path).convert('RGB')
            prompt = "a high quality X-ray image of scoliosis with nopadding"
        else:
            # åŠ è½½å¢å¼ºå›¾
            aug_idx = idx - self.num_original
            img_path = os.path.join(self.augmented_dir, self.augmented_files[aug_idx])
            image = Image.open(img_path).convert('RGB')
            prompt = "a high quality X-ray image of scoliosis with padding"
        
        # Resize åˆ°ç›®æ ‡åˆ†è¾¨ç‡
        image = image.resize((self.resolution, self.resolution), Image.LANCZOS)
        
        # è½¬æ¢ä¸ºtensorï¼Œå½’ä¸€åŒ–åˆ° [-1, 1]
        image = torch.from_numpy(np.array(image)).permute(2, 0, 1).float() / 127.5 - 1.0
        
        # Tokenize prompt
        tokens_1 = self.tokenizer_1(
            prompt, padding="max_length", max_length=77,
            truncation=True, return_tensors="pt"
        )
        tokens_2 = self.tokenizer_2(
            prompt, padding="max_length", max_length=77,
            truncation=True, return_tensors="pt"
        )
        
        return {
            "pixel_values": image,
            "input_ids_1": tokens_1.input_ids[0],
            "attention_mask_1": tokens_1.attention_mask[0],
            "input_ids_2": tokens_2.input_ids[0],
            "attention_mask_2": tokens_2.attention_mask[0],
        }


def encode_prompt(text_encoder_1, text_encoder_2,
                  input_ids_1, input_ids_2,
                  attention_mask_1=None, attention_mask_2=None):
    """ç¼–ç æ–‡æœ¬ prompt"""
    out1 = text_encoder_1(input_ids_1, attention_mask=attention_mask_1, 
                          output_hidden_states=False, return_dict=True)
    out2 = text_encoder_2(input_ids_2, attention_mask=attention_mask_2,
                          output_hidden_states=False, return_dict=True)

    prompt_embeds_1 = out1.last_hidden_state
    prompt_embeds_2 = out2.last_hidden_state

    if hasattr(out1, "text_embeds") and out1.text_embeds is not None:
        pooled_1 = out1.text_embeds
    elif hasattr(out1, "pooler_output") and out1.pooler_output is not None:
        pooled_1 = out1.pooler_output
    else:
        pooled_1 = prompt_embeds_1[:, 0, :]

    if hasattr(out2, "text_embeds") and out2.text_embeds is not None:
        pooled_2 = out2.text_embeds
    elif hasattr(out2, "pooler_output") and out2.pooler_output is not None:
        pooled_2 = out2.pooler_output
    else:
        pooled_2 = prompt_embeds_2[:, 0, :]

    prompt_embeds = torch.cat([prompt_embeds_1, prompt_embeds_2], dim=-1)
    pooled_prompt_embeds = torch.cat([pooled_1, pooled_2], dim=-1)

    return prompt_embeds, pooled_prompt_embeds


def adapt_transformer_for_two_encoders(transformer, new_in_features=2048):
    """é€‚é… transformer çš„ context_embedder"""
    old_embedder = transformer.context_embedder
    old_out_features = old_embedder.out_features
    old_in_features = old_embedder.in_features
    
    if old_in_features == new_in_features:
        logger.info(f"âš ï¸  context_embedder already adapted to {new_in_features}")
        return transformer
    
    logger.info(f"ğŸ”§ Adapting context_embedder: {old_in_features} â†’ {new_in_features}")
    
    new_embedder = torch.nn.Linear(
        new_in_features, old_out_features,
        bias=old_embedder.bias is not None,
        dtype=old_embedder.weight.dtype,
        device=old_embedder.weight.device
    )
    
    with torch.no_grad():
        if old_in_features > new_in_features:
            new_embedder.weight.data = old_embedder.weight.data[:, :new_in_features].clone()
        else:
            new_embedder.weight.data[:, :old_in_features] = old_embedder.weight.data.clone()
            torch.nn.init.xavier_uniform_(new_embedder.weight.data[:, old_in_features:])
        
        if new_embedder.bias is not None and old_embedder.bias is not None:
            new_embedder.bias.data = old_embedder.bias.data.clone()
    
    transformer.context_embedder = new_embedder
    logger.info(f"âœ… Context embedder adapted successfully")
    return transformer


def apply_lora_to_transformer(transformer, lora_rank=16, lora_alpha=16):
    """åº”ç”¨ LoRA åˆ° transformer"""
    logger.info(f"ğŸ¯ Applying LoRA (rank={lora_rank}, alpha={lora_alpha})")
    
    lora_config = LoraConfig(
        r=lora_rank,
        lora_alpha=lora_alpha,
        init_lora_weights="gaussian",
        target_modules=["to_q", "to_k", "to_v", "to_out.0"],
        lora_dropout=0.1,
    )
    
    transformer = get_peft_model(transformer, lora_config)
    
    trainable = sum(p.numel() for p in transformer.parameters() if p.requires_grad)
    total = sum(p.numel() for p in transformer.parameters())
    
    logger.info(f"   Trainable: {trainable:,} ({100*trainable/total:.2f}%)")
    logger.info(f"   Total: {total:,}")
    
    return transformer


def generate_validation_image(vae, text_encoder_1, text_encoder_2, transformer, 
                             tokenizer_1, tokenizer_2, scheduler_config,
                             validation_prompt, output_path, step, epoch, 
                             device, weight_dtype, current_loss=None):
    """ç”ŸæˆéªŒè¯å›¾ç‰‡"""
    logger.info(f"ğŸ“¸ Generating validation image at step {step}...")
    
    transformer.eval()
    inference_scheduler = DDPMScheduler.from_config(scheduler_config)
    
    try:
        with torch.no_grad():
            tokens_1 = tokenizer_1(validation_prompt, padding="max_length", 
                                  max_length=77, truncation=True, return_tensors="pt")
            tokens_2 = tokenizer_2(validation_prompt, padding="max_length",
                                  max_length=77, truncation=True, return_tensors="pt")
            
            prompt_embeds, pooled_prompt_embeds = encode_prompt(
                text_encoder_1, text_encoder_2,
                tokens_1.input_ids.to(device), tokens_2.input_ids.to(device),
                tokens_1.attention_mask.to(device), tokens_2.attention_mask.to(device)
            )
            
            # æ¯æ¬¡ä½¿ç”¨éšæœºç§å­ç”Ÿæˆä¸åŒçš„éªŒè¯å›¾ç‰‡
            generator = torch.Generator(device=device)
            latents = torch.randn((1, 16, 128, 128), generator=generator,
                                 device=device, dtype=weight_dtype)
            
            inference_scheduler.set_timesteps(28)
            latents = latents * inference_scheduler.init_noise_sigma
            
            for t in inference_scheduler.timesteps:
                noise_pred = transformer(
                    hidden_states=latents, timestep=t.unsqueeze(0).to(device),
                    encoder_hidden_states=prompt_embeds,
                    pooled_projections=pooled_prompt_embeds,
                    return_dict=False
                )[0]
                latents = inference_scheduler.step(noise_pred, t, latents).prev_sample
            
            # ğŸ”§ ä¿®å¤ï¼šè§£ç åˆ°åƒç´ ç©ºé—´ï¼Œç¡®ä¿æ•°æ®ç±»å‹åŒ¹é…
            latents = latents / vae.config.scaling_factor
            # å°† latents è½¬æ¢ä¸ºä¸ VAE ç›¸åŒçš„æ•°æ®ç±»å‹
            latents = latents.to(dtype=weight_dtype)
            image = vae.decode(latents).sample
            
            # è½¬æ¢åˆ° PIL Image
            image = (image / 2 + 0.5).clamp(0, 1)
            image = image.cpu().permute(0, 2, 3, 1).float().numpy()
            image = (image[0] * 255).round().astype("uint8")
            image = Image.fromarray(image)
        
        timestamp = datetime.datetime.now().strftime("%H%M%S")
        if current_loss is not None:
            filename = f"step{step:06d}_epoch{epoch:03d}_loss{current_loss:.4f}_{timestamp}.png"
        else:
            filename = f"step{step:06d}_epoch{epoch:03d}_{timestamp}.png"
        
        save_path = os.path.join(output_path, filename)
        image.save(save_path)
        logger.info(f"âœ“ Saved: {save_path}")
    
    except Exception as e:
        logger.error(f"âœ— Failed to generate validation image: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        transformer.train()


def train(args):
    # ğŸš€ ä½¿ç”¨ Accelerator
    accelerator = Accelerator(
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        mixed_precision=args.mixed_precision,
        log_with="tensorboard",
        project_dir=os.path.join(args.output_dir, "logs"),
    )
    
    # è®¾ç½®éšæœºç§å­ï¼ˆå¯é€‰ï¼‰
    if args.seed is not None:
        set_seed(args.seed)
        logger.info(f"âœ“ Using fixed seed: {args.seed}")
    else:
        logger.info(f"âœ“ Using random seed")
    
    # åŠ è½½æ¨¡å‹
    logger.info("Loading SD3 model...")
    
    if accelerator.mixed_precision == "fp16":
        weight_dtype = torch.float16
    elif accelerator.mixed_precision == "bf16":
        weight_dtype = torch.bfloat16
    else:
        weight_dtype = torch.float32
    
    pipe = StableDiffusion3Pipeline.from_single_file(
        args.safetensors_path,
        config=args.sd3_config_dir,
        torch_dtype=weight_dtype,
        local_files_only=True,
        text_encoder_3=None,
        tokenizer_3=None,
    )
    
    # æå–ç»„ä»¶
    vae = pipe.vae
    text_encoder_1 = pipe.text_encoder
    text_encoder_2 = pipe.text_encoder_2
    transformer = pipe.transformer
    
    noise_scheduler = DDPMScheduler.from_config(pipe.scheduler.config)
    
    logger.info(f"âœ“ Using DDPMScheduler")
    logger.info(f"âœ“ VAE scaling_factor: {vae.config.scaling_factor}")
    
    # é€‚é… transformer
    transformer = adapt_transformer_for_two_encoders(transformer, new_in_features=2048)
    
    # åº”ç”¨ LoRA
    if args.use_lora:
        transformer = apply_lora_to_transformer(
            transformer, 
            lora_rank=args.lora_rank,
            lora_alpha=args.lora_alpha
        )
    else:
        transformer.requires_grad_(True)
        logger.info(f"Training full transformer")
    
    # å†»ç»“ä¸è®­ç»ƒçš„æ¨¡å‹
    vae.requires_grad_(False)
    text_encoder_1.requires_grad_(False)
    text_encoder_2.requires_grad_(False)
    
    # æ¢¯åº¦æ£€æŸ¥ç‚¹
    if args.gradient_checkpointing:
        transformer.enable_gradient_checkpointing()
        logger.info("âœ“ Gradient checkpointing enabled")
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = XrayDatasetDual(
        args.instance_data_dir,
        pipe.tokenizer,
        pipe.tokenizer_2,
        resolution=args.resolution
    )
    
    dataloader = DataLoader(
        dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=args.num_workers,
        pin_memory=True
    )
    
    # ä¼˜åŒ–å™¨
    if args.use_8bit_adam:
        try:
            import bitsandbytes as bnb
            optimizer_class = bnb.optim.AdamW8bit
            logger.info("âœ“ Using 8-bit Adam optimizer")
        except ImportError:
            logger.warning("bitsandbytes not found, using standard AdamW")
            optimizer_class = torch.optim.AdamW
    else:
        optimizer_class = torch.optim.AdamW
    
    trainable_params = [p for p in transformer.parameters() if p.requires_grad]
    optimizer = optimizer_class(
        trainable_params,
        lr=args.learning_rate,
        betas=(args.adam_beta1, args.adam_beta2),
        weight_decay=args.adam_weight_decay,
        eps=args.adam_epsilon,
    )
    
    # è®¡ç®—è®­ç»ƒæ­¥æ•°
    num_update_steps_per_epoch = math.ceil(len(dataloader) / args.gradient_accumulation_steps)
    if args.max_train_steps is None:
        args.max_train_steps = args.epochs * num_update_steps_per_epoch
    else:
        args.epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    lr_scheduler = get_scheduler(
        args.lr_scheduler,
        optimizer=optimizer,
        num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,
        num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,
    )
    
    # ä½¿ç”¨ Accelerator å‡†å¤‡æ¨¡å‹
    transformer, optimizer, dataloader, lr_scheduler = accelerator.prepare(
        transformer, optimizer, dataloader, lr_scheduler
    )
    
    # ç§»åŠ¨å…¶ä»–æ¨¡å‹åˆ°è®¾å¤‡
    vae.to(accelerator.device, dtype=weight_dtype)
    text_encoder_1.to(accelerator.device, dtype=weight_dtype)
    text_encoder_2.to(accelerator.device, dtype=weight_dtype)
    
    # æ‰“å°è®­ç»ƒé…ç½®
    total_batch_size = args.batch_size * accelerator.num_processes * args.gradient_accumulation_steps
    
    logger.info("\n" + "="*60)
    logger.info("***** Training Configuration *****")
    logger.info(f"  Num samples = {len(dataset)}")
    logger.info(f"  Num epochs = {args.epochs}")
    logger.info(f"  Instantaneous batch size = {args.batch_size}")
    logger.info(f"  Total train batch size = {total_batch_size}")
    logger.info(f"  Gradient accumulation steps = {args.gradient_accumulation_steps}")
    logger.info(f"  Total optimization steps = {args.max_train_steps}")
    logger.info(f"  Learning rate = {args.learning_rate}")
    logger.info(f"  LR scheduler = {args.lr_scheduler}")
    logger.info(f"  Use LoRA = {args.use_lora}")
    if args.use_lora:
        logger.info(f"  LoRA rank = {args.lora_rank}")
    logger.info("="*60 + "\n")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    if accelerator.is_main_process:
        os.makedirs(args.output_dir, exist_ok=True)
        validation_dir = os.path.join(args.output_dir, "validation_images")
        os.makedirs(validation_dir, exist_ok=True)
    
    # åˆå§‹åŒ– tracker
    if accelerator.is_main_process:
        accelerator.init_trackers("sd3_xray_training")
    
    # ç”Ÿæˆåˆå§‹éªŒè¯å›¾ç‰‡
    if accelerator.is_main_process:
        generate_validation_image(
            vae, text_encoder_1, text_encoder_2,
            accelerator.unwrap_model(transformer),
            pipe.tokenizer, pipe.tokenizer_2, noise_scheduler.config,
            args.validation_prompt, validation_dir, 0, 0,
            accelerator.device, weight_dtype
        )
    
    # è®­ç»ƒå¾ªç¯
    global_step = 0
    first_epoch = 0
    
    progress_bar = tqdm(
        range(0, args.max_train_steps),
        initial=0,
        desc="Steps",
        disable=not accelerator.is_local_main_process,
    )
    
    for epoch in range(first_epoch, args.epochs):
        transformer.train()
        epoch_loss = 0
        
        for step, batch in enumerate(dataloader):
            with accelerator.accumulate(transformer):
                pixel_values = batch["pixel_values"].to(dtype=weight_dtype)
                
                # ç¼–ç 
                with torch.no_grad():
                    latents = vae.encode(pixel_values).latent_dist.sample()
                    latents = latents * vae.config.scaling_factor
                    
                    prompt_embeds, pooled_prompt_embeds = encode_prompt(
                        text_encoder_1, text_encoder_2,
                        batch["input_ids_1"], batch["input_ids_2"],
                        batch["attention_mask_1"], batch["attention_mask_2"]
                    )
                
                # æ·»åŠ å™ªå£°
                noise = torch.randn_like(latents)
                bsz = latents.shape[0]
                timesteps = torch.randint(
                    0, noise_scheduler.config.num_train_timesteps,
                    (bsz,), device=latents.device
                ).long()
                
                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)
                
                # é¢„æµ‹å™ªå£°
                model_pred = transformer(
                    hidden_states=noisy_latents,
                    timestep=timesteps,
                    encoder_hidden_states=prompt_embeds,
                    pooled_projections=pooled_prompt_embeds,
                    return_dict=False
                )[0]
                
                # è®¡ç®—æŸå¤±
                loss = F.mse_loss(model_pred.float(), noise.float(), reduction="mean")
                
                # åå‘ä¼ æ’­
                accelerator.backward(loss)
                
                if accelerator.sync_gradients:
                    accelerator.clip_grad_norm_(trainable_params, args.max_grad_norm)
                
                optimizer.step()
                lr_scheduler.step()
                optimizer.zero_grad()
            
            # æ›´æ–°è¿›åº¦
            if accelerator.sync_gradients:
                progress_bar.update(1)
                global_step += 1
                
                # è®°å½•æ—¥å¿—
                epoch_loss += loss.detach().item()
                
                logs = {
                    "loss": loss.detach().item(),
                    "lr": lr_scheduler.get_last_lr()[0],
                    "epoch": epoch,
                }
                progress_bar.set_postfix(**logs)
                accelerator.log(logs, step=global_step)
                
                # ç”ŸæˆéªŒè¯å›¾ç‰‡
                if global_step % args.validation_steps == 0 and accelerator.is_main_process:
                    generate_validation_image(
                        vae, text_encoder_1, text_encoder_2,
                        accelerator.unwrap_model(transformer),
                        pipe.tokenizer, pipe.tokenizer_2, noise_scheduler.config,
                        args.validation_prompt, validation_dir, global_step, epoch + 1,
                        accelerator.device, weight_dtype,
                        current_loss=epoch_loss / (step + 1)
                    )
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                if global_step % args.save_steps == 0 and accelerator.is_main_process:
                    save_path = os.path.join(args.output_dir, f"checkpoint-{global_step}")
                    os.makedirs(save_path, exist_ok=True)
                    
                    unwrapped_transformer = accelerator.unwrap_model(transformer)
                    unwrapped_transformer.save_pretrained(save_path)
                    
                    logger.info(f"\nâœ“ Saved checkpoint to {save_path}")
                
                if global_step >= args.max_train_steps:
                    break
        
        # Epoch ç»“æŸ
        avg_epoch_loss = epoch_loss / len(dataloader)
        logger.info(f"\nEpoch {epoch + 1}/{args.epochs} - Avg Loss: {avg_epoch_loss:.6f}")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    if accelerator.is_main_process:
        final_path = os.path.join(args.output_dir, "final_lora" if args.use_lora else "final_transformer")
        os.makedirs(final_path, exist_ok=True)
        
        unwrapped_transformer = accelerator.unwrap_model(transformer)
        unwrapped_transformer.save_pretrained(final_path)
        
        logger.info(f"\n{'='*60}")
        logger.info(f"âœ“ Training complete!")
        logger.info(f"âœ“ Final model saved to: {final_path}")
        logger.info(f"{'='*60}")
    
    accelerator.end_training()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="SD3 Fine-tuning with pre-augmented data")
    
    # æ¨¡å‹å’Œæ•°æ®
    parser.add_argument("--safetensors_path", type=str, required=True,
                       help="Path to SD3 safetensors checkpoint")
    parser.add_argument("--sd3_config_dir", type=str, required=True,
                       help="Path to SD3 config directory")
    parser.add_argument("--instance_data_dir", type=str, required=True,
                       help="Root dir containing train/X-ray and train/X-ray_transform_padding512512")
    parser.add_argument("--output_dir", type=str, default="./output",
                       help="Output directory for checkpoints")
    parser.add_argument("--validation_prompt", type=str, 
                       default="a high quality X-ray image of scoliosis",
                       help="Prompt for validation image generation")
    
    # è®­ç»ƒè®¾ç½®
    parser.add_argument("--resolution", type=int, default=1024,
                       help="Image resolution")
    parser.add_argument("--batch_size", type=int, default=1,
                       help="Training batch size")
    parser.add_argument("--epochs", type=int, default=500,
                       help="Number of training epochs")
    parser.add_argument("--max_train_steps", type=int, default=None,
                       help="Total number of training steps (overrides epochs)")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=8,
                       help="Number of gradient accumulation steps")
    parser.add_argument("--gradient_checkpointing", action="store_true",
                       help="Enable gradient checkpointing to save memory")
    parser.add_argument("--mixed_precision", type=str, default="bf16",
                       choices=["no", "fp16", "bf16"],
                       help="Mixed precision training")
    
    # ä¼˜åŒ–å™¨è®¾ç½®
    parser.add_argument("--learning_rate", type=float, default=1e-4,
                       help="Learning rate")
    parser.add_argument("--lr_scheduler", type=str, default="cosine",
                       choices=["linear", "cosine", "cosine_with_restarts", "constant", "constant_with_warmup"],
                       help="Learning rate scheduler type")
    parser.add_argument("--lr_warmup_steps", type=int, default=500,
                       help="Number of warmup steps for learning rate scheduler")
    parser.add_argument("--use_8bit_adam", action="store_true",
                       help="Use 8-bit Adam optimizer to save memory")
    parser.add_argument("--adam_beta1", type=float, default=0.9,
                       help="Adam beta1")
    parser.add_argument("--adam_beta2", type=float, default=0.999,
                       help="Adam beta2")
    parser.add_argument("--adam_weight_decay", type=float, default=1e-2,
                       help="Adam weight decay")
    parser.add_argument("--adam_epsilon", type=float, default=1e-8,
                       help="Adam epsilon")
    parser.add_argument("--max_grad_norm", type=float, default=1.0,
                       help="Max gradient norm for clipping")
    
    # LoRA è®¾ç½®
    parser.add_argument("--use_lora", action="store_true",
                       help="Use LoRA instead of full fine-tuning")
    parser.add_argument("--lora_rank", type=int, default=16,
                       help="LoRA rank")
    parser.add_argument("--lora_alpha", type=int, default=16,
                       help="LoRA alpha")
    
    # å…¶ä»–
    parser.add_argument("--seed", type=int, default=None,
                       help="Random seed for reproducibility (optional)")
    parser.add_argument("--num_workers", type=int, default=2,
                       help="Number of dataloader workers")
    parser.add_argument("--save_steps", type=int, default=4000,
                       help="Save checkpoint every X steps")
    parser.add_argument("--validation_steps", type=int, default=500,
                       help="Generate validation image every X steps")
    
    args = parser.parse_args()
    train(args)
